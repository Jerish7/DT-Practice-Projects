{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> The sonar dataset consists of 208 rows, where 111 rows are the patterns obtained by bouncing sonar signals off a mine(metal cylinder) at various angles and under various conditions, whereas the remaining 97 rows are the patterns obtained from rocks under similar conditions. \n",
    "\n",
    "-> The transmitted sonar signal is a frequency-modulated chirp, rising in frequency. \n",
    "\n",
    "-> The data set contains signals obtained from a variety of different aspect angles, spanning 90 degrees for the cylinder and 180 degrees for the rock.\n",
    "\n",
    "-> Each pattern is a set of 60 numbers in the range 0.0 to 1.0, which are the total number of columns.\n",
    "\n",
    "-> Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp.\n",
    "\n",
    "-> The label associated with each record contains the letter “R” if the object is a rock and “M” if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly.\n",
    "\n",
    "-> We need to classify whether the objects are either rocks or mines and also we need to find the prediction. we will approach classification process for this problem statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing warning library to avoid any warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9   ...      51      52      53      54      55      56      57  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "       58      59  60  \n",
       "0  0.0090  0.0032   R  \n",
       "1  0.0052  0.0044   R  \n",
       "2  0.0095  0.0078   R  \n",
       "3  0.0040  0.0117   R  \n",
       "4  0.0107  0.0094   R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('D:/Python file/Project Datasets/sonar_dataset.csv',header=None)  #Path location of the dataset \n",
    "df.head()  #Checking out the top 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 61)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape  #Checking out the dimensions of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 208 rows and 61 columns as we gave header=None so that the index data has been added inside the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.038437</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.075202</td>\n",
       "      <td>0.104570</td>\n",
       "      <td>0.121747</td>\n",
       "      <td>0.134799</td>\n",
       "      <td>0.178003</td>\n",
       "      <td>0.208259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>0.006507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>0.055552</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>0.061788</td>\n",
       "      <td>0.085152</td>\n",
       "      <td>0.118387</td>\n",
       "      <td>0.134416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.005031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.067025</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.080425</td>\n",
       "      <td>0.097025</td>\n",
       "      <td>0.111275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.044050</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>0.106950</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.152250</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.006850</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.035550</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.057950</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.100275</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020825</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.008525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.710600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.043900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.029164    0.038437    0.043832    0.053892    0.075202    0.104570   \n",
       "std      0.022991    0.032960    0.038428    0.046528    0.055552    0.059105   \n",
       "min      0.001500    0.000600    0.001500    0.005800    0.006700    0.010200   \n",
       "25%      0.013350    0.016450    0.018950    0.024375    0.038050    0.067025   \n",
       "50%      0.022800    0.030800    0.034300    0.044050    0.062500    0.092150   \n",
       "75%      0.035550    0.047950    0.057950    0.064500    0.100275    0.134125   \n",
       "max      0.137100    0.233900    0.305900    0.426400    0.401000    0.382300   \n",
       "\n",
       "               6           7           8           9   ...          50  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  ...  208.000000   \n",
       "mean     0.121747    0.134799    0.178003    0.208259  ...    0.016069   \n",
       "std      0.061788    0.085152    0.118387    0.134416  ...    0.012008   \n",
       "min      0.003300    0.005500    0.007500    0.011300  ...    0.000000   \n",
       "25%      0.080900    0.080425    0.097025    0.111275  ...    0.008425   \n",
       "50%      0.106950    0.112100    0.152250    0.182400  ...    0.013900   \n",
       "75%      0.154000    0.169600    0.233425    0.268700  ...    0.020825   \n",
       "max      0.372900    0.459000    0.682800    0.710600  ...    0.100400   \n",
       "\n",
       "               51          52          53          54          55          56  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.013420    0.010709    0.010941    0.009290    0.008222    0.007820   \n",
       "std      0.009634    0.007060    0.007301    0.007088    0.005736    0.005785   \n",
       "min      0.000800    0.000500    0.001000    0.000600    0.000400    0.000300   \n",
       "25%      0.007275    0.005075    0.005375    0.004150    0.004400    0.003700   \n",
       "50%      0.011400    0.009550    0.009300    0.007500    0.006850    0.005950   \n",
       "75%      0.016725    0.014900    0.014500    0.012100    0.010575    0.010425   \n",
       "max      0.070900    0.039000    0.035200    0.044700    0.039400    0.035500   \n",
       "\n",
       "               57          58          59  \n",
       "count  208.000000  208.000000  208.000000  \n",
       "mean     0.007949    0.007941    0.006507  \n",
       "std      0.006470    0.006181    0.005031  \n",
       "min      0.000300    0.000100    0.000600  \n",
       "25%      0.003600    0.003675    0.003100  \n",
       "50%      0.005800    0.006400    0.005300  \n",
       "75%      0.010350    0.010325    0.008525  \n",
       "max      0.044000    0.036400    0.043900  \n",
       "\n",
       "[8 rows x 60 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking out the statistical summary of the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1.There are no missing values in the dataset.\n",
    "\n",
    "2.There is not much difference between the mean and median so that we can estimate that there are no outliers present. But still, we can check the dataset whether there are outliers or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 208 entries, 0 to 207\n",
      "Data columns (total 61 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       208 non-null    float64\n",
      " 1   1       208 non-null    float64\n",
      " 2   2       208 non-null    float64\n",
      " 3   3       208 non-null    float64\n",
      " 4   4       208 non-null    float64\n",
      " 5   5       208 non-null    float64\n",
      " 6   6       208 non-null    float64\n",
      " 7   7       208 non-null    float64\n",
      " 8   8       208 non-null    float64\n",
      " 9   9       208 non-null    float64\n",
      " 10  10      208 non-null    float64\n",
      " 11  11      208 non-null    float64\n",
      " 12  12      208 non-null    float64\n",
      " 13  13      208 non-null    float64\n",
      " 14  14      208 non-null    float64\n",
      " 15  15      208 non-null    float64\n",
      " 16  16      208 non-null    float64\n",
      " 17  17      208 non-null    float64\n",
      " 18  18      208 non-null    float64\n",
      " 19  19      208 non-null    float64\n",
      " 20  20      208 non-null    float64\n",
      " 21  21      208 non-null    float64\n",
      " 22  22      208 non-null    float64\n",
      " 23  23      208 non-null    float64\n",
      " 24  24      208 non-null    float64\n",
      " 25  25      208 non-null    float64\n",
      " 26  26      208 non-null    float64\n",
      " 27  27      208 non-null    float64\n",
      " 28  28      208 non-null    float64\n",
      " 29  29      208 non-null    float64\n",
      " 30  30      208 non-null    float64\n",
      " 31  31      208 non-null    float64\n",
      " 32  32      208 non-null    float64\n",
      " 33  33      208 non-null    float64\n",
      " 34  34      208 non-null    float64\n",
      " 35  35      208 non-null    float64\n",
      " 36  36      208 non-null    float64\n",
      " 37  37      208 non-null    float64\n",
      " 38  38      208 non-null    float64\n",
      " 39  39      208 non-null    float64\n",
      " 40  40      208 non-null    float64\n",
      " 41  41      208 non-null    float64\n",
      " 42  42      208 non-null    float64\n",
      " 43  43      208 non-null    float64\n",
      " 44  44      208 non-null    float64\n",
      " 45  45      208 non-null    float64\n",
      " 46  46      208 non-null    float64\n",
      " 47  47      208 non-null    float64\n",
      " 48  48      208 non-null    float64\n",
      " 49  49      208 non-null    float64\n",
      " 50  50      208 non-null    float64\n",
      " 51  51      208 non-null    float64\n",
      " 52  52      208 non-null    float64\n",
      " 53  53      208 non-null    float64\n",
      " 54  54      208 non-null    float64\n",
      " 55  55      208 non-null    float64\n",
      " 56  56      208 non-null    float64\n",
      " 57  57      208 non-null    float64\n",
      " 58  58      208 non-null    float64\n",
      " 59  59      208 non-null    float64\n",
      " 60  60      208 non-null    object \n",
      "dtypes: float64(60), object(1)\n",
      "memory usage: 99.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()    #Checking the datatype of all the columns present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is only one categorical data, which is the target variable, we need to encode it and leave the other columns as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "     ..\n",
       "56    0\n",
       "57    0\n",
       "58    0\n",
       "59    0\n",
       "60    0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()  #Checking whether there are missing values or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are no NaN/Missing values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Principal Component Analysis to reduce data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dataset is big and we need to compress the data. This can be done by using PCA process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Separating independent and dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9   ...      50      51      52      53      54      55      56  \\\n",
       "0  0.2111  ...  0.0232  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180   \n",
       "1  0.2872  ...  0.0125  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140   \n",
       "2  0.6194  ...  0.0033  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316   \n",
       "3  0.1264  ...  0.0241  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050   \n",
       "4  0.4459  ...  0.0156  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072   \n",
       "\n",
       "       57      58      59  \n",
       "0  0.0084  0.0090  0.0032  \n",
       "1  0.0049  0.0052  0.0044  \n",
       "2  0.0164  0.0095  0.0078  \n",
       "3  0.0044  0.0040  0.0117  \n",
       "4  0.0048  0.0107  0.0094  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df.iloc[:,0:-1]   #Separating independent variables using iloc function\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 60)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape   #Dimensions of dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      R\n",
       "1      R\n",
       "2      R\n",
       "3      R\n",
       "4      R\n",
       "      ..\n",
       "203    M\n",
       "204    M\n",
       "205    M\n",
       "206    M\n",
       "207    M\n",
       "Name: 60, Length: 208, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=df.iloc[:,-1]   #Dependent variable/Target variable\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable consists of 'R'(Rocks) and 'M'(Mines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Reduce x data using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing PCA from decomposition module\n",
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=10) #We are compressing 60 columns into 10 columns\n",
    "x=pca.fit_transform(x)  #Transforming x data into PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.576093</td>\n",
       "      <td>0.319393</td>\n",
       "      <td>-0.387291</td>\n",
       "      <td>-0.378009</td>\n",
       "      <td>-0.243780</td>\n",
       "      <td>-0.156243</td>\n",
       "      <td>-0.427736</td>\n",
       "      <td>0.074832</td>\n",
       "      <td>-0.024347</td>\n",
       "      <td>0.318054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.970667</td>\n",
       "      <td>1.018039</td>\n",
       "      <td>0.834156</td>\n",
       "      <td>0.107423</td>\n",
       "      <td>0.286070</td>\n",
       "      <td>0.178684</td>\n",
       "      <td>-0.260332</td>\n",
       "      <td>0.385600</td>\n",
       "      <td>-0.004030</td>\n",
       "      <td>-0.072529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.325887</td>\n",
       "      <td>0.874209</td>\n",
       "      <td>0.246015</td>\n",
       "      <td>0.863031</td>\n",
       "      <td>0.069110</td>\n",
       "      <td>-0.300833</td>\n",
       "      <td>-0.412807</td>\n",
       "      <td>0.075321</td>\n",
       "      <td>-0.010855</td>\n",
       "      <td>-0.438559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.852035</td>\n",
       "      <td>0.690863</td>\n",
       "      <td>-0.013390</td>\n",
       "      <td>-0.110257</td>\n",
       "      <td>-0.554050</td>\n",
       "      <td>0.397968</td>\n",
       "      <td>0.492569</td>\n",
       "      <td>-0.220460</td>\n",
       "      <td>0.537003</td>\n",
       "      <td>-0.436795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.064756</td>\n",
       "      <td>0.222968</td>\n",
       "      <td>0.638601</td>\n",
       "      <td>0.140432</td>\n",
       "      <td>0.135854</td>\n",
       "      <td>0.052881</td>\n",
       "      <td>-0.386558</td>\n",
       "      <td>0.066549</td>\n",
       "      <td>-0.117006</td>\n",
       "      <td>-0.419277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.041781</td>\n",
       "      <td>-1.031644</td>\n",
       "      <td>0.265669</td>\n",
       "      <td>0.021516</td>\n",
       "      <td>-0.015439</td>\n",
       "      <td>0.097304</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.177926</td>\n",
       "      <td>-0.027908</td>\n",
       "      <td>-0.264179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.219149</td>\n",
       "      <td>-1.153807</td>\n",
       "      <td>0.203619</td>\n",
       "      <td>0.045836</td>\n",
       "      <td>0.183272</td>\n",
       "      <td>0.012770</td>\n",
       "      <td>0.078507</td>\n",
       "      <td>0.264831</td>\n",
       "      <td>0.070749</td>\n",
       "      <td>-0.303296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.297733</td>\n",
       "      <td>-1.151860</td>\n",
       "      <td>0.250096</td>\n",
       "      <td>-0.057702</td>\n",
       "      <td>0.163566</td>\n",
       "      <td>0.015275</td>\n",
       "      <td>0.045792</td>\n",
       "      <td>0.225904</td>\n",
       "      <td>0.072130</td>\n",
       "      <td>-0.313044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>-0.045761</td>\n",
       "      <td>-0.995970</td>\n",
       "      <td>0.199677</td>\n",
       "      <td>-0.066516</td>\n",
       "      <td>-0.147227</td>\n",
       "      <td>-0.084600</td>\n",
       "      <td>0.031038</td>\n",
       "      <td>0.067444</td>\n",
       "      <td>0.025771</td>\n",
       "      <td>-0.171120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>-0.051115</td>\n",
       "      <td>-0.912526</td>\n",
       "      <td>0.008910</td>\n",
       "      <td>-0.044387</td>\n",
       "      <td>-0.027915</td>\n",
       "      <td>-0.029949</td>\n",
       "      <td>0.033107</td>\n",
       "      <td>-0.020868</td>\n",
       "      <td>-0.118992</td>\n",
       "      <td>-0.144134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.576093  0.319393 -0.387291 -0.378009 -0.243780 -0.156243 -0.427736   \n",
       "1   -0.970667  1.018039  0.834156  0.107423  0.286070  0.178684 -0.260332   \n",
       "2   -0.325887  0.874209  0.246015  0.863031  0.069110 -0.300833 -0.412807   \n",
       "3    0.852035  0.690863 -0.013390 -0.110257 -0.554050  0.397968  0.492569   \n",
       "4   -0.064756  0.222968  0.638601  0.140432  0.135854  0.052881 -0.386558   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "203  0.041781 -1.031644  0.265669  0.021516 -0.015439  0.097304  0.003784   \n",
       "204  0.219149 -1.153807  0.203619  0.045836  0.183272  0.012770  0.078507   \n",
       "205  0.297733 -1.151860  0.250096 -0.057702  0.163566  0.015275  0.045792   \n",
       "206 -0.045761 -0.995970  0.199677 -0.066516 -0.147227 -0.084600  0.031038   \n",
       "207 -0.051115 -0.912526  0.008910 -0.044387 -0.027915 -0.029949  0.033107   \n",
       "\n",
       "            7         8         9  \n",
       "0    0.074832 -0.024347  0.318054  \n",
       "1    0.385600 -0.004030 -0.072529  \n",
       "2    0.075321 -0.010855 -0.438559  \n",
       "3   -0.220460  0.537003 -0.436795  \n",
       "4    0.066549 -0.117006 -0.419277  \n",
       "..        ...       ...       ...  \n",
       "203  0.177926 -0.027908 -0.264179  \n",
       "204  0.264831  0.070749 -0.303296  \n",
       "205  0.225904  0.072130 -0.313044  \n",
       "206  0.067444  0.025771 -0.171120  \n",
       "207 -0.020868 -0.118992 -0.144134  \n",
       "\n",
       "[208 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=pd.DataFrame(data=x)  #Coverting transformed data into dataframe\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As dependent variable consists of categorical data, we need to encode the data and it can be done by using LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()   #Instance of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=le.fit_transform(y)   #Encoding the data from category to numeric\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here 1 represents R(Rocks) whereas 0 represents M(Mines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking skewness of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.177056\n",
       "1    0.403361\n",
       "2    0.179856\n",
       "3    0.437246\n",
       "4   -0.081498\n",
       "5    0.260802\n",
       "6    0.407128\n",
       "7    0.058413\n",
       "8   -0.223131\n",
       "9   -0.266651\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no skewness of data present as it is in the range -0.5 to +0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the percentage of data falling under outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([146, 164, 171], dtype=int64), array([9, 6, 7], dtype=int64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can use zscore for checking the percentage\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "z=np.abs(zscore(x))\n",
    "threshold=3\n",
    "np.where(z>3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.576093</td>\n",
       "      <td>0.319393</td>\n",
       "      <td>-0.387291</td>\n",
       "      <td>-0.378009</td>\n",
       "      <td>-0.243780</td>\n",
       "      <td>-0.156243</td>\n",
       "      <td>-0.427736</td>\n",
       "      <td>0.074832</td>\n",
       "      <td>-0.024347</td>\n",
       "      <td>0.318054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.970667</td>\n",
       "      <td>1.018039</td>\n",
       "      <td>0.834156</td>\n",
       "      <td>0.107423</td>\n",
       "      <td>0.286070</td>\n",
       "      <td>0.178684</td>\n",
       "      <td>-0.260332</td>\n",
       "      <td>0.385600</td>\n",
       "      <td>-0.004030</td>\n",
       "      <td>-0.072529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.325887</td>\n",
       "      <td>0.874209</td>\n",
       "      <td>0.246015</td>\n",
       "      <td>0.863031</td>\n",
       "      <td>0.069110</td>\n",
       "      <td>-0.300833</td>\n",
       "      <td>-0.412807</td>\n",
       "      <td>0.075321</td>\n",
       "      <td>-0.010855</td>\n",
       "      <td>-0.438559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.852035</td>\n",
       "      <td>0.690863</td>\n",
       "      <td>-0.013390</td>\n",
       "      <td>-0.110257</td>\n",
       "      <td>-0.554050</td>\n",
       "      <td>0.397968</td>\n",
       "      <td>0.492569</td>\n",
       "      <td>-0.220460</td>\n",
       "      <td>0.537003</td>\n",
       "      <td>-0.436795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.064756</td>\n",
       "      <td>0.222968</td>\n",
       "      <td>0.638601</td>\n",
       "      <td>0.140432</td>\n",
       "      <td>0.135854</td>\n",
       "      <td>0.052881</td>\n",
       "      <td>-0.386558</td>\n",
       "      <td>0.066549</td>\n",
       "      <td>-0.117006</td>\n",
       "      <td>-0.419277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.041781</td>\n",
       "      <td>-1.031644</td>\n",
       "      <td>0.265669</td>\n",
       "      <td>0.021516</td>\n",
       "      <td>-0.015439</td>\n",
       "      <td>0.097304</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.177926</td>\n",
       "      <td>-0.027908</td>\n",
       "      <td>-0.264179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.219149</td>\n",
       "      <td>-1.153807</td>\n",
       "      <td>0.203619</td>\n",
       "      <td>0.045836</td>\n",
       "      <td>0.183272</td>\n",
       "      <td>0.012770</td>\n",
       "      <td>0.078507</td>\n",
       "      <td>0.264831</td>\n",
       "      <td>0.070749</td>\n",
       "      <td>-0.303296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.297733</td>\n",
       "      <td>-1.151860</td>\n",
       "      <td>0.250096</td>\n",
       "      <td>-0.057702</td>\n",
       "      <td>0.163566</td>\n",
       "      <td>0.015275</td>\n",
       "      <td>0.045792</td>\n",
       "      <td>0.225904</td>\n",
       "      <td>0.072130</td>\n",
       "      <td>-0.313044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>-0.045761</td>\n",
       "      <td>-0.995970</td>\n",
       "      <td>0.199677</td>\n",
       "      <td>-0.066516</td>\n",
       "      <td>-0.147227</td>\n",
       "      <td>-0.084600</td>\n",
       "      <td>0.031038</td>\n",
       "      <td>0.067444</td>\n",
       "      <td>0.025771</td>\n",
       "      <td>-0.171120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>-0.051115</td>\n",
       "      <td>-0.912526</td>\n",
       "      <td>0.008910</td>\n",
       "      <td>-0.044387</td>\n",
       "      <td>-0.027915</td>\n",
       "      <td>-0.029949</td>\n",
       "      <td>0.033107</td>\n",
       "      <td>-0.020868</td>\n",
       "      <td>-0.118992</td>\n",
       "      <td>-0.144134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.576093  0.319393 -0.387291 -0.378009 -0.243780 -0.156243 -0.427736   \n",
       "1   -0.970667  1.018039  0.834156  0.107423  0.286070  0.178684 -0.260332   \n",
       "2   -0.325887  0.874209  0.246015  0.863031  0.069110 -0.300833 -0.412807   \n",
       "3    0.852035  0.690863 -0.013390 -0.110257 -0.554050  0.397968  0.492569   \n",
       "4   -0.064756  0.222968  0.638601  0.140432  0.135854  0.052881 -0.386558   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "203  0.041781 -1.031644  0.265669  0.021516 -0.015439  0.097304  0.003784   \n",
       "204  0.219149 -1.153807  0.203619  0.045836  0.183272  0.012770  0.078507   \n",
       "205  0.297733 -1.151860  0.250096 -0.057702  0.163566  0.015275  0.045792   \n",
       "206 -0.045761 -0.995970  0.199677 -0.066516 -0.147227 -0.084600  0.031038   \n",
       "207 -0.051115 -0.912526  0.008910 -0.044387 -0.027915 -0.029949  0.033107   \n",
       "\n",
       "            7         8         9  \n",
       "0    0.074832 -0.024347  0.318054  \n",
       "1    0.385600 -0.004030 -0.072529  \n",
       "2    0.075321 -0.010855 -0.438559  \n",
       "3   -0.220460  0.537003 -0.436795  \n",
       "4    0.066549 -0.117006 -0.419277  \n",
       "..        ...       ...       ...  \n",
       "203  0.177926 -0.027908 -0.264179  \n",
       "204  0.264831  0.070749 -0.303296  \n",
       "205  0.225904  0.072130 -0.313044  \n",
       "206  0.067444  0.025771 -0.171120  \n",
       "207 -0.020868 -0.118992 -0.144134  \n",
       "\n",
       "[205 rows x 10 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing outliers\n",
    "df_new=x[(z<3).all(axis=1)]\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  #Original dataset dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape  #New dataset dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing outliers\n",
    "df_new1=y[(z<3).all(axis=1)]\n",
    "df_new1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 rows had outliers and they are removed by using zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentage loss of data while removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4423076923076923\n"
     ]
    }
   ],
   "source": [
    "percentage_loss=((3)/208)*100\n",
    "print(percentage_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage loss of data is very less and we don't need to remove outliers further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes model can be biased to higher values in dataset, so it is better to scale the dataset so that we can bring all the columns in common range. We can use StandardScaler here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.576093</td>\n",
       "      <td>0.319393</td>\n",
       "      <td>-0.387291</td>\n",
       "      <td>-0.378009</td>\n",
       "      <td>-0.243780</td>\n",
       "      <td>-0.156243</td>\n",
       "      <td>-0.427736</td>\n",
       "      <td>0.074832</td>\n",
       "      <td>-0.024347</td>\n",
       "      <td>0.318054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.970667</td>\n",
       "      <td>1.018039</td>\n",
       "      <td>0.834156</td>\n",
       "      <td>0.107423</td>\n",
       "      <td>0.286070</td>\n",
       "      <td>0.178684</td>\n",
       "      <td>-0.260332</td>\n",
       "      <td>0.385600</td>\n",
       "      <td>-0.004030</td>\n",
       "      <td>-0.072529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.325887</td>\n",
       "      <td>0.874209</td>\n",
       "      <td>0.246015</td>\n",
       "      <td>0.863031</td>\n",
       "      <td>0.069110</td>\n",
       "      <td>-0.300833</td>\n",
       "      <td>-0.412807</td>\n",
       "      <td>0.075321</td>\n",
       "      <td>-0.010855</td>\n",
       "      <td>-0.438559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.852035</td>\n",
       "      <td>0.690863</td>\n",
       "      <td>-0.013390</td>\n",
       "      <td>-0.110257</td>\n",
       "      <td>-0.554050</td>\n",
       "      <td>0.397968</td>\n",
       "      <td>0.492569</td>\n",
       "      <td>-0.220460</td>\n",
       "      <td>0.537003</td>\n",
       "      <td>-0.436795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.064756</td>\n",
       "      <td>0.222968</td>\n",
       "      <td>0.638601</td>\n",
       "      <td>0.140432</td>\n",
       "      <td>0.135854</td>\n",
       "      <td>0.052881</td>\n",
       "      <td>-0.386558</td>\n",
       "      <td>0.066549</td>\n",
       "      <td>-0.117006</td>\n",
       "      <td>-0.419277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.576093  0.319393 -0.387291 -0.378009 -0.243780 -0.156243 -0.427736   \n",
       "1 -0.970667  1.018039  0.834156  0.107423  0.286070  0.178684 -0.260332   \n",
       "2 -0.325887  0.874209  0.246015  0.863031  0.069110 -0.300833 -0.412807   \n",
       "3  0.852035  0.690863 -0.013390 -0.110257 -0.554050  0.397968  0.492569   \n",
       "4 -0.064756  0.222968  0.638601  0.140432  0.135854  0.052881 -0.386558   \n",
       "\n",
       "          7         8         9  \n",
       "0  0.074832 -0.024347  0.318054  \n",
       "1  0.385600 -0.004030 -0.072529  \n",
       "2  0.075321 -0.010855 -0.438559  \n",
       "3 -0.220460  0.537003 -0.436795  \n",
       "4  0.066549 -0.117006 -0.419277  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df_new\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=df_new1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "scaledX=sc.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required metrices and model for the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating train_test_split with random_state\n",
    "x_train,x_test,y_train,y_test=train_test_split(scaledX,y,test_size=.20,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding out the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "  0.6341463414634146\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "  [[13  9]\n",
      " [ 6 13]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.59      0.63        22\n",
      "           1       0.59      0.68      0.63        19\n",
      "\n",
      "    accuracy                           0.63        41\n",
      "   macro avg       0.64      0.64      0.63        41\n",
      "weighted avg       0.64      0.63      0.63        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "lr.score(x_train,y_train)\n",
    "pred_lr=lr.predict(x_test)  \n",
    "print('Accuracy score:\\n ',accuracy_score(y_test,pred_lr))\n",
    "print('\\n')\n",
    "print('Confusion Matrix:\\n ',confusion_matrix(y_test,pred_lr))\n",
    "print('\\n')\n",
    "print('Classification report:\\n')\n",
    "print(classification_report(y_test,pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "  0.6585365853658537\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "  [[14  8]\n",
      " [ 6 13]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.64      0.67        22\n",
      "           1       0.62      0.68      0.65        19\n",
      "\n",
      "    accuracy                           0.66        41\n",
      "   macro avg       0.66      0.66      0.66        41\n",
      "weighted avg       0.66      0.66      0.66        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb=GaussianNB()\n",
    "gnb.fit(x_train,y_train)\n",
    "gnb.score(x_train,y_train)\n",
    "predgnb=gnb.predict(x_test)\n",
    "print('Accuracy score:\\n ',accuracy_score(y_test,predgnb))\n",
    "print('\\n')\n",
    "print('Confusion Matrix:\\n ',confusion_matrix(y_test,predgnb))\n",
    "print('\\n')\n",
    "print('Classification report:\\n')\n",
    "print(classification_report(y_test,predgnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "  0.7073170731707317\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "  [[13  9]\n",
      " [ 3 16]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.59      0.68        22\n",
      "           1       0.64      0.84      0.73        19\n",
      "\n",
      "    accuracy                           0.71        41\n",
      "   macro avg       0.73      0.72      0.71        41\n",
      "weighted avg       0.73      0.71      0.70        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc=DecisionTreeClassifier()\n",
    "dtc.fit(x_train,y_train)\n",
    "dtc.score(x_train,y_train)\n",
    "preddtc=dtc.predict(x_test)\n",
    "print('Accuracy score:\\n ',accuracy_score(y_test,preddtc))\n",
    "print('\\n')\n",
    "print('Confusion Matrix:\\n ',confusion_matrix(y_test,preddtc))\n",
    "print('\\n')\n",
    "print('Classification report:\\n')\n",
    "print(classification_report(y_test,preddtc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "  0.7317073170731707\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "  [[14  8]\n",
      " [ 3 16]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.64      0.72        22\n",
      "           1       0.67      0.84      0.74        19\n",
      "\n",
      "    accuracy                           0.73        41\n",
      "   macro avg       0.75      0.74      0.73        41\n",
      "weighted avg       0.75      0.73      0.73        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(x_train,y_train)\n",
    "rf.score(x_train,y_train)\n",
    "predrf=rf.predict(x_test)\n",
    "print('Accuracy score:\\n ',accuracy_score(y_test,predrf))\n",
    "print('\\n')\n",
    "print('Confusion Matrix:\\n ',confusion_matrix(y_test,predrf))\n",
    "print('\\n')\n",
    "print('Classification report:\\n')\n",
    "print(classification_report(y_test,predrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above model, we can say that RandomForestClassifier has more accuracy compared to other models and we will perform further process with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting AUC-ROC curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(x_train,y_train)\n",
    "p=rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Random Forest Curve\n",
    "y_pred_prob=rf.predict_proba(x_test)[:,0]\n",
    "tpr,fpr,thresholds=roc_curve(y_test,y_pred_prob)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.plot(fpr,tpr,label='Random Forest Classifier')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('Random Forest Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7392344497607656"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding auc_score\n",
    "auc_score=roc_auc_score(y_test,rf.predict(x_test))\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating cross-validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.36585366 0.68292683 0.41463415 0.75609756 0.41463415]\n",
      "Mean value of the scores:  0.526829268292683\n",
      "Standard Deviation:  0.1600118972376266\n"
     ]
    }
   ],
   "source": [
    "rfscores=cross_val_score(rf,x,y,cv=5,scoring='accuracy')    #For Random Forest Classifier\n",
    "print('Scores:',rfscores)\n",
    "print('Mean value of the scores: ',rfscores.mean())\n",
    "print('Standard Deviation: ',rfscores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A technique used to find out the best parameter for our model to improve the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating parameters list\n",
    "parameters={'n_estimators':[10,100,500,1000],'criterion' :['gini', 'entropy']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'n_estimators': 500}\n",
      "0.8231060606060605\n"
     ]
    }
   ],
   "source": [
    "#Using GridSearchCV to run the parameters and checking final accuracy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "GCV=GridSearchCV(RandomForestClassifier(),parameters,cv=5,scoring='accuracy')\n",
    "#Initializing GridSearchCV\n",
    "\n",
    "GCV.fit(x_train,y_train)\n",
    "print(GCV.best_params_)\n",
    "print(GCV.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Random Forest on CV data:  0.7804878048780488\n"
     ]
    }
   ],
   "source": [
    "rfc1=RandomForestClassifier(random_state=42,n_estimators=10,criterion='gini')\n",
    "rfc1.fit(x_train,y_train)\n",
    "pred=rfc1.predict(x_test)\n",
    "print(\"Accuracy for Random Forest on CV data: \",accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After hyperparameter tuning, we can see that accuracy has been increased from 73% to 78% and hence the model is fit with cv score nearly 52% and accuracy 78%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SonarDatasetProject.pkl']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rfc1,'SonarDatasetProject.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
